{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pxuanbach/recommendation-system/blob/main/Demo_model_based_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkVZOmF3-KrU"
      },
      "source": [
        "# Demo model-based approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4qLPOvD59jg"
      },
      "source": [
        "### 1. Cài đặt và thêm các package cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PUJKdcLs6Prd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\back\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\back\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyspark) (0.10.9.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\back\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-1.5.2-cp39-cp39-win_amd64.whl (10.9 MB)\n",
            "Collecting numpy>=1.20.3; python_version < \"3.10\"\n",
            "  Using cached numpy-1.24.0-cp39-cp39-win_amd64.whl (14.9 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\back\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
            "Collecting pytz>=2020.1\n",
            "  Using cached pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\back\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Installing collected packages: numpy, pytz, pandas\n",
            "Successfully installed numpy-1.24.0 pandas-1.5.2 pytz-2022.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\back\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IOhmBy5-5wG2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, explode\n",
        "from pyspark import SparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2OocY7r6hjx"
      },
      "source": [
        "Tạo spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AbYXcI4T6kEB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Recommendations Demo').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj2XSDbe6rm4"
      },
      "source": [
        "### 2. Tải dữ liệu lên"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QPWPI0-s6tlG"
      },
      "outputs": [],
      "source": [
        "movies = spark.read.csv('movies.csv', header=True)\n",
        "ratings = spark.read.csv('ratings.csv', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D9qxR2l6wOc",
        "outputId": "e7c05fd0-ff2f-4fc5-e686-189574347148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+------+---------+\n",
            "|userId|movieId|rating|timestamp|\n",
            "+------+-------+------+---------+\n",
            "|     1|      1|   4.0|964982703|\n",
            "|     1|      3|   4.0|964981247|\n",
            "|     1|      6|   4.0|964982224|\n",
            "|     1|     47|   5.0|964983815|\n",
            "|     1|     50|   5.0|964982931|\n",
            "|     1|     70|   3.0|964982400|\n",
            "|     1|    101|   5.0|964980868|\n",
            "|     1|    110|   4.0|964982176|\n",
            "|     1|    151|   5.0|964984041|\n",
            "|     1|    157|   5.0|964984100|\n",
            "|     1|    163|   5.0|964983650|\n",
            "|     1|    216|   5.0|964981208|\n",
            "|     1|    223|   3.0|964980985|\n",
            "|     1|    231|   5.0|964981179|\n",
            "|     1|    235|   4.0|964980908|\n",
            "|     1|    260|   5.0|964981680|\n",
            "|     1|    296|   3.0|964982967|\n",
            "|     1|    316|   3.0|964982310|\n",
            "|     1|    333|   5.0|964981179|\n",
            "|     1|    349|   4.0|964982563|\n",
            "+------+-------+------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ratings.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6XUM0p799Z6"
      },
      "source": [
        "Chỉnh sửa schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svk8bTOL9_Qu",
        "outputId": "020f6852-1b35-410b-92d6-f5768ba529f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+------+\n",
            "|userId|movieId|rating|\n",
            "+------+-------+------+\n",
            "|     1|      1|   4.0|\n",
            "|     1|      3|   4.0|\n",
            "|     1|      6|   4.0|\n",
            "|     1|     47|   5.0|\n",
            "|     1|     50|   5.0|\n",
            "|     1|     70|   3.0|\n",
            "|     1|    101|   5.0|\n",
            "|     1|    110|   4.0|\n",
            "|     1|    151|   5.0|\n",
            "|     1|    157|   5.0|\n",
            "|     1|    163|   5.0|\n",
            "|     1|    216|   5.0|\n",
            "|     1|    223|   3.0|\n",
            "|     1|    231|   5.0|\n",
            "|     1|    235|   4.0|\n",
            "|     1|    260|   5.0|\n",
            "|     1|    296|   3.0|\n",
            "|     1|    316|   3.0|\n",
            "|     1|    333|   5.0|\n",
            "|     1|    349|   4.0|\n",
            "+------+-------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ratings = ratings.\\\n",
        "    withColumn('userId', col('userId').cast('integer')).\\\n",
        "    withColumn('movieId', col('movieId').cast('integer')).\\\n",
        "    withColumn('rating', col('rating').cast('float')).\\\n",
        "    drop('timestamp')\n",
        "ratings.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQDtz_Yk-DKi"
      },
      "source": [
        "### 3. Tính toán độ thưa thớt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4zE7bNx-ebP",
        "outputId": "4bf91a0b-3c50-4e8f-8587-2c853581e06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of ratings 40022\n",
            "Number users 274 Number movies 6222\n",
            "The ratings dataframe is  97.65% empty.\n"
          ]
        }
      ],
      "source": [
        "# Count the total number of ratings in the dataset\n",
        "numerator = ratings.select(\"rating\").count()\n",
        "print(\"Total number of ratings\", numerator)\n",
        "\n",
        "# Count the number of distinct userIds and distinct movieIds\n",
        "num_users = ratings.select(\"userId\").distinct().count()\n",
        "num_movies = ratings.select(\"movieId\").distinct().count()\n",
        "print(\"Number users\", num_users, \"Number movies\", num_movies)\n",
        "\n",
        "# Set the denominator equal to the number of users multiplied by the number of movies\n",
        "denominator = num_users * num_movies\n",
        "\n",
        "# Divide the numerator by the denominator\n",
        "sparsity = (1.0 - (numerator *1.0)/denominator)*100\n",
        "print(\"The ratings dataframe is \", \"%.2f\" % sparsity + \"% empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqNtN4r4_JvI"
      },
      "source": [
        "### 4. Nhóm dữ liệu bằng cách tính tổng số lượt ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klPpJWqd_fdZ"
      },
      "source": [
        "Nhóm user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42FweGtO_Vxc",
        "outputId": "9f13c320-affb-4819-ab2e-5d371b9c294c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+\n",
            "|userId|count|\n",
            "+------+-----+\n",
            "|    68| 1260|\n",
            "|   249| 1046|\n",
            "|   182|  977|\n",
            "|   177|  904|\n",
            "|   232|  862|\n",
            "|   274|  793|\n",
            "|   105|  722|\n",
            "|    19|  703|\n",
            "|   111|  646|\n",
            "|   217|  613|\n",
            "|   140|  608|\n",
            "|    91|  575|\n",
            "|    28|  570|\n",
            "|   219|  528|\n",
            "|    89|  518|\n",
            "|    64|  517|\n",
            "|   226|  507|\n",
            "|    18|  502|\n",
            "|    57|  476|\n",
            "|    21|  443|\n",
            "+------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Group data by userId, count ratings\n",
        "userId_ratings = ratings.groupBy(\"userId\").count().orderBy('count', ascending=False)\n",
        "userId_ratings.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i2Ida0c_htS"
      },
      "source": [
        "Nhóm movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4gif2y4_jCU",
        "outputId": "d5f748bb-f41c-4764-f58c-8af8ae4aa6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|movieId|count|\n",
            "+-------+-----+\n",
            "|    296|  144|\n",
            "|    356|  144|\n",
            "|    318|  140|\n",
            "|   2571|  129|\n",
            "|    593|  125|\n",
            "|    260|  113|\n",
            "|    110|  104|\n",
            "|    480|  103|\n",
            "|   1196|  102|\n",
            "|    589|  100|\n",
            "|      1|   99|\n",
            "|   1210|   96|\n",
            "|   1198|   96|\n",
            "|    780|   95|\n",
            "|     47|   95|\n",
            "|    150|   94|\n",
            "|   2858|   90|\n",
            "|    527|   90|\n",
            "|    592|   89|\n",
            "|   2028|   89|\n",
            "+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Group data by movieId, count ratings\n",
        "movieId_ratings = ratings.groupBy(\"movieId\").count().orderBy('count', ascending=False)\n",
        "movieId_ratings.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtC_2X2R_uMB"
      },
      "source": [
        "### 5. Xây dựng mô hình ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ppyuetvv_xt_"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.recommendation import ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zawHWfta_1CN",
        "outputId": "96709a37-c325-473d-cf72-49e585147501"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.ml.recommendation.ALS"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create test and train set\n",
        "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 1234)\n",
        "\n",
        "# Create ALS model\n",
        "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative = True, implicitPrefs = False, coldStartStrategy=\"drop\")\n",
        "\n",
        "# Confirm that a model called \"als\" was created\n",
        "type(als)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9tvd4YjAS_C"
      },
      "source": [
        "Điều chỉnh siêu tham số cho ALS model\n",
        "- rank (rank of the factorization) hạng của thừa số hóa.\n",
        "- regParam (regularization parameter (>= 0)) là tham số chính quy hóa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A9xpWNz7AYJi"
      },
      "outputs": [],
      "source": [
        "# Import the requisite items\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Add hyperparameters and their respective values to param_grid\n",
        "param_grid = ParamGridBuilder() \\\n",
        "            .addGrid(als.rank, [10, 50, 100]) \\\n",
        "            .addGrid(als.regParam, [.01, .05, .1]) \\\n",
        "            .build()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHsB3l2jBkI0"
      },
      "source": [
        "Sau đó tạo RegressionEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBBy-sSSByxe",
        "outputId": "f868d05e-25a6-49b9-8f45-300c7e039cb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num models to be tested:  9\n"
          ]
        }
      ],
      "source": [
        "# Define evaluator as RMSE and print length of evaluator\n",
        "evaluator = RegressionEvaluator(\n",
        "           metricName=\"rmse\", \n",
        "           labelCol=\"rating\", \n",
        "           predictionCol=\"prediction\") \n",
        "print (\"Num models to be tested: \", len(param_grid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tJU7DDNICAco"
      },
      "outputs": [],
      "source": [
        "# Build cross validation using CrossValidator\n",
        "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNGBqNMCELD"
      },
      "source": [
        "### 6. Kiểm tra các thông số được lựa chọn cho mô hình tốt nhất"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zLvQzplNCbWd"
      },
      "outputs": [],
      "source": [
        "#Fit cross validator to the 'train' dataset\n",
        "model = cv.fit(train)\n",
        "\n",
        "#Extract best model from the cv model above\n",
        "best_model = model.bestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omgksmj8CTto",
        "outputId": "b7e647b1-0577-4e33-affe-b12d28b249f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Best Model**\n",
            "  Rank: 10\n",
            "  MaxIter: 10\n",
            "  RegParam: 0.1\n"
          ]
        }
      ],
      "source": [
        "print(\"**Best Model**\")\n",
        "# Print \"Rank\"\n",
        "print(\"  Rank:\", best_model._java_obj.parent().getRank())\n",
        "# Print \"MaxIter\"\n",
        "print(\"  MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
        "# Print \"RegParam\"\n",
        "print(\"  RegParam:\", best_model._java_obj.parent().getRegParam())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZCF1FsqCo65"
      },
      "source": [
        "Xem thử dự đoán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9nU-kEwCquS",
        "outputId": "ab69343b-2c31-48d8-e916-e5d568239887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9496723878260326\n"
          ]
        }
      ],
      "source": [
        "# View the predictions\n",
        "test_predictions = best_model.transform(test)\n",
        "RMSE = evaluator.evaluate(test_predictions)\n",
        "print(RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-SxQo9TflRr",
        "outputId": "c9f805b1-7096-4d95-cc51-1cb0b492ba9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+------+----------+\n",
            "|userId|movieId|rating|prediction|\n",
            "+------+-------+------+----------+\n",
            "|   148|    356|   4.0|  3.854917|\n",
            "|   148|   4896|   4.0| 3.3067513|\n",
            "|   148|   4993|   3.0|  3.066208|\n",
            "|   148|   7153|   3.0| 3.6244917|\n",
            "|   148|   8368|   4.0| 3.7092826|\n",
            "|   148|  40629|   5.0| 2.7161942|\n",
            "|   148|  50872|   3.0| 3.0652325|\n",
            "|   148|  60069|   4.5| 3.7623978|\n",
            "|   148|  69757|   3.5| 3.7207644|\n",
            "|   148|  72998|   4.0| 3.1236095|\n",
            "|   148|  81847|   4.5|  3.531046|\n",
            "|   148|  98491|   5.0| 3.5566206|\n",
            "|   148| 115617|   3.5|  4.017053|\n",
            "|   148| 122886|   3.5| 3.8015947|\n",
            "|   243|     62|   5.0| 4.0809207|\n",
            "|   243|    145|   4.0| 4.7360873|\n",
            "|   243|    248|   4.0|  4.787063|\n",
            "|   243|    520|   5.0| 3.8411067|\n",
            "|   243|    589|   4.0| 4.7180047|\n",
            "|   243|    592|   3.0|  4.171158|\n",
            "+------+-------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_predictions.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI-l1ETPfvjZ"
      },
      "source": [
        "### 7. Đưa ra đề xuất"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EVfMbAff4Iv",
        "outputId": "8ca7d523-edd4-4835-e6dd-715ebb65a411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+\n",
            "|userId|     recommendations|\n",
            "+------+--------------------+\n",
            "|     1|[{1658, 6.2070546...|\n",
            "|     2|[{1658, 5.0419745...|\n",
            "|     3|[{70946, 4.968466...|\n",
            "|     4|[{720, 5.5889835}...|\n",
            "|     5|[{213, 5.071801},...|\n",
            "|     6|[{150548, 5.47501...|\n",
            "|     7|[{47423, 4.947241...|\n",
            "|     8|[{28, 5.1227274},...|\n",
            "|     9|[{1658, 5.741402}...|\n",
            "|    10|[{71579, 4.868856...|\n",
            "+------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate n Recommendations for all users\n",
        "nrecommendations = best_model.recommendForAllUsers(10)\n",
        "nrecommendations.limit(10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXM4ziP4f-0s",
        "outputId": "129aa071-9ec0-4dad-d954-b167506ba00c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+---------+\n",
            "|userId|movieId|   rating|\n",
            "+------+-------+---------+\n",
            "|     1|   1658|6.2070546|\n",
            "|     1|    898|5.6630154|\n",
            "|     1|   1250| 5.568542|\n",
            "|     1|   1262| 5.562644|\n",
            "|     1|   7842| 5.527656|\n",
            "|     1|  44195|5.4760265|\n",
            "|     1|  55118| 5.418627|\n",
            "|     1|   3275| 5.413618|\n",
            "|     1|   5466| 5.406734|\n",
            "|     1|   1408| 5.396342|\n",
            "+------+-------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nrecommendations = nrecommendations\\\n",
        "    .withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
        "    .select('userId', col(\"rec_exp.movieId\"), col(\"rec_exp.rating\"))\n",
        "\n",
        "nrecommendations.limit(10).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZg_HQ5NgRDp"
      },
      "source": [
        "### 8. Các đề xuất có hợp lý không?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvrtI-hvgS3d",
        "outputId": "6ef2601a-8856-41ef-9702-66394ab14b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+---------+--------------------+--------------------+\n",
            "|movieId|userId|   rating|               title|              genres|\n",
            "+-------+------+---------+--------------------+--------------------+\n",
            "| 122886|   100|5.2960615|Star Wars: Episod...|Action|Adventure|...|\n",
            "|   1262|   100| 5.058984|Great Escape, The...|Action|Adventure|...|\n",
            "|  69134|   100|  4.97764|   Antichrist (2009)|       Drama|Fantasy|\n",
            "|   3200|   100| 4.960183|Last Detail, The ...|        Comedy|Drama|\n",
            "| 147330|   100|4.9516926|Sherlock Holmes a...|               Crime|\n",
            "| 104780|   100|4.9516926|Mystery of the Th...|Adventure|Animati...|\n",
            "| 173351|   100|4.9516926|Wow! A Talking Fi...|Animation|Childre...|\n",
            "| 159811|   100|4.9516926|The Bremen Town M...|Animation|Drama|F...|\n",
            "| 142020|   100|4.9516926|        Oscar (1967)|              Comedy|\n",
            "|   2131|   100|4.9516926|Autumn Sonata (Hö...|               Drama|\n",
            "+-------+------+---------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nrecommendations.join(movies, on='movieId').filter('userId = 100').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpt52f6pgZww",
        "outputId": "3d9294e3-4420-4253-b980-76804087441f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+------+--------------------+--------------------+\n",
            "|movieId|userId|rating|               title|              genres|\n",
            "+-------+------+------+--------------------+--------------------+\n",
            "|   2423|   100|   5.0|Christmas Vacatio...|              Comedy|\n",
            "|   1101|   100|   5.0|      Top Gun (1986)|      Action|Romance|\n",
            "|   4041|   100|   5.0|Officer and a Gen...|       Drama|Romance|\n",
            "|   1958|   100|   5.0|Terms of Endearme...|        Comedy|Drama|\n",
            "|   5620|   100|   5.0|Sweet Home Alabam...|      Comedy|Romance|\n",
            "|    919|   100|   4.5|Wizard of Oz, The...|Adventure|Childre...|\n",
            "|    934|   100|   4.5|Father of the Bri...|              Comedy|\n",
            "|     28|   100|   4.5|   Persuasion (1995)|       Drama|Romance|\n",
            "|     95|   100|   4.5| Broken Arrow (1996)|Action|Adventure|...|\n",
            "|   1028|   100|   4.5| Mary Poppins (1964)|Children|Comedy|F...|\n",
            "|   1091|   100|   4.5|Weekend at Bernie...|              Comedy|\n",
            "|     16|   100|   4.5|       Casino (1995)|         Crime|Drama|\n",
            "|   1246|   100|   4.5|Dead Poets Societ...|               Drama|\n",
            "|   1278|   100|   4.5|Young Frankenstei...|      Comedy|Fantasy|\n",
            "|   1500|   100|   4.5|Grosse Pointe Bla...|Comedy|Crime|Romance|\n",
            "|    261|   100|   4.5| Little Women (1994)|               Drama|\n",
            "|    539|   100|   4.5|Sleepless in Seat...|Comedy|Drama|Romance|\n",
            "|    597|   100|   4.5| Pretty Woman (1990)|      Comedy|Romance|\n",
            "|    838|   100|   4.5|         Emma (1996)|Comedy|Drama|Romance|\n",
            "|   1441|   100|   4.5| Benny & Joon (1993)|      Comedy|Romance|\n",
            "+-------+------+------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ratings.join(movies, on='movieId').filter('userId = 100').sort('rating', ascending=False).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Lưu model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o13708.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelWriter.saveImpl(ALS.scala:539)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rfPath \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m best_model\u001b[39m.\u001b[39;49msave(rfPath)\n",
            "File \u001b[1;32mc:\\Users\\Back\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\ml\\util.py:246\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     \u001b[39m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite()\u001b[39m.\u001b[39;49msave(path)\n",
            "File \u001b[1;32mc:\\Users\\Back\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\ml\\util.py:197\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpath should be a string, got type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(path))\n\u001b[1;32m--> 197\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
            "File \u001b[1;32mc:\\Users\\Back\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[1;32mc:\\Users\\Back\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\Back\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o13708.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelWriter.saveImpl(ALS.scala:539)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n"
          ]
        }
      ],
      "source": [
        "save_model_path = \"trained\"\n",
        "best_model.save(path=save_model_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNJtyr0ysUF/Z6cNQ5t1L9v",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "5f0f3744a15961e348356f9827a4657b97c13f71aaaccbaf1aecedcda23e9227"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
